{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e56633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e154ff",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34efcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, pooling='avg'):\n",
    "        super(SSE, self).__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.phi(x)\n",
    "        if self.pooling == 'avg':\n",
    "            x = torch.mean(x, dim=1)\n",
    "        elif self.pooling == 'max':\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pooling == 'sum':\n",
    "            x = torch.sum(x, dim=1)\n",
    "        \n",
    "        return self.rho(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89afad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    torch.randn(32, 100),\n",
    "    torch.randn(16, 100),\n",
    "]\n",
    "\n",
    "X = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "mask = torch.arange(X.size(1))[None, :] < torch.tensor([len(b) for b in batch])[:, None]\n",
    "X = X * mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2816bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "model = SSE(input_size=100, hidden_size=256, output_size=128, pooling='avg')\n",
    "output = model(X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b99d63",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239e4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self, z_dim, element_dim, max_elements):\n",
    "        super(SSD, self).__init__()\n",
    "        self.max_elements = max_elements\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(z_dim*2, element_dim * max_elements),\n",
    "        )\n",
    "        self.element_dim = element_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.decoder(x)\n",
    "        out = out.view(-1, self.max_elements, self.element_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b3898",
   "metadata": {},
   "source": [
    "# S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af1d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, z_dim, element_dim, max_elements, output_size):\n",
    "        super(S2S, self).__init__()\n",
    "        self.encoder = SSE(input_size, hidden_size, z_dim, pooling='avg')\n",
    "        self.decoder = SSD(z_dim, element_dim, max_elements)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4715cc",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c82724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticSetDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, min_len=3, max_len=8, elem_dim=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.elem_dim = elem_dim\n",
    "\n",
    "        # Pre-generate sets\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            # Ensure n and m are Python ints (not PyTorch Number/float) so torch.randn accepts them\n",
    "            n = int(torch.randint(min_len, max_len+1, (1,)).item())\n",
    "            m = int(torch.randint(min_len, max_len+1, (1,)).item())\n",
    "            A = torch.randn(n, elem_dim)           # Modality A\n",
    "            # Create B by sampling rows from A so B is correlated and has shape (m, elem_dim).\n",
    "            # Use without-replacement when m <= n, otherwise sample with replacement.\n",
    "            if m <= n:\n",
    "                indices = torch.randperm(n)[:m]\n",
    "            else:\n",
    "                indices = torch.randint(0, n, (m,))\n",
    "            B = A[indices] + 0.1 * torch.randn(m, elem_dim)\n",
    "            self.data.append((A, B))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch, max_elements_decoder=8):\n",
    "    \"\"\"\n",
    "    Pads variable-length sets to:\n",
    "      - input: max length in batch\n",
    "      - output: decoder max_elements\n",
    "    Returns:\n",
    "      A_padded: [B, max_n, D]\n",
    "      B_padded: [B, max_elements_decoder, D]\n",
    "      A_mask, B_mask: boolean masks\n",
    "    \"\"\"\n",
    "    As, Bs = zip(*batch)\n",
    "    B_size = max_elements_decoder\n",
    "    D = As[0].size(1)\n",
    "    max_n = max([a.size(0) for a in As])\n",
    "\n",
    "    B_padded = torch.zeros(len(batch), B_size, D)\n",
    "    A_padded = torch.zeros(len(batch), max_n, D)\n",
    "    A_mask = torch.zeros(len(batch), max_n).bool()\n",
    "    B_mask = torch.zeros(len(batch), B_size).bool()\n",
    "\n",
    "    for i, (a, b) in enumerate(zip(As, Bs)):\n",
    "        # Encode input set\n",
    "        A_padded[i, :a.size(0), :] = a\n",
    "        A_mask[i, :a.size(0)] = 1\n",
    "\n",
    "        # Pad output set to decoder size\n",
    "        num_b = min(b.size(0), B_size)\n",
    "        B_padded[i, :num_b, :] = b[:num_b]\n",
    "        B_mask[i, :num_b] = 1\n",
    "\n",
    "    return A_padded, B_padded, A_mask, B_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c5e44",
   "metadata": {},
   "source": [
    "# CHAMFER LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b511c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_loss(pred, target, pred_mask, target_mask):\n",
    "    \"\"\"\n",
    "    pred: [B, N, D]  (decoder output)\n",
    "    target: [B, M, D] (padded/truncated to N=M=decoder max)\n",
    "    pred_mask, target_mask: boolean masks [B, N]\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances\n",
    "    diff = pred.unsqueeze(2) - target.unsqueeze(1)  # [B, N, N, D]\n",
    "    dist = torch.norm(diff, dim=-1)                 # [B, N, N]\n",
    "\n",
    "    # Mask invalid positions\n",
    "    pred_mask = pred_mask.unsqueeze(2)              # [B, N, 1]\n",
    "    target_mask = target_mask.unsqueeze(1)          # [B, 1, N]\n",
    "    valid_mask = pred_mask & target_mask\n",
    "    dist_masked = dist.clone()\n",
    "    dist_masked[~valid_mask] = float('inf')\n",
    "\n",
    "    # Chamfer distance: nearest neighbor\n",
    "    min_dist_pred = dist_masked.min(dim=2)[0]       # [B, N]\n",
    "    min_dist_target = dist_masked.min(dim=1)[0]     # [B, N]\n",
    "\n",
    "    # Average only valid elements\n",
    "    loss = (min_dist_pred[pred_mask.squeeze(2)].mean() +\n",
    "            min_dist_target[target_mask.squeeze(1)].mean())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844b0c9",
   "metadata": {},
   "source": [
    "# DEFINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358a2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "hidden_size = 64\n",
    "z_dim = 128\n",
    "element_dim = 100\n",
    "max_elements = 8  # max output set size\n",
    "output_size = element_dim  # same as element dim\n",
    "\n",
    "model = S2S(input_size, hidden_size, z_dim, element_dim, max_elements, output_size)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f376886",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5566ff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 20.8733\n",
      "Epoch 2/10, Loss: 20.6866\n",
      "Epoch 3/10, Loss: 20.6844\n",
      "Epoch 4/10, Loss: 20.6649\n",
      "Epoch 5/10, Loss: 20.6808\n",
      "Epoch 6/10, Loss: 20.6679\n",
      "Epoch 7/10, Loss: 20.6699\n",
      "Epoch 8/10, Loss: 20.6899\n",
      "Epoch 9/10, Loss: 20.6688\n",
      "Epoch 10/10, Loss: 20.6840\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "dataset = SyntheticSetDataset(num_samples=1000, min_len=3, max_len=8, elem_dim=100)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, \n",
    "                        collate_fn=lambda batch: collate_fn(batch, max_elements_decoder=8))\n",
    "\n",
    "# Model\n",
    "input_size = 100\n",
    "hidden_size = 256\n",
    "z_dim = 256\n",
    "element_dim = 100\n",
    "max_elements = 8\n",
    "output_size = element_dim\n",
    "\n",
    "model = S2S(input_size, hidden_size, z_dim, element_dim, max_elements, output_size).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for A, B, A_mask, B_mask in dataloader:\n",
    "        A, B = A.cuda(), B.cuda()\n",
    "        A_mask, B_mask = A_mask.cuda(), B_mask.cuda()\n",
    "        A = (A-A.mean(dim=1, keepdim=True))/A.std(dim=1, keepdim=True)\n",
    "        B = (B-B.mean(dim=1, keepdim=True))/B.std(dim=1, keepdim=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        B_hat = model(A)  # [B, max_elements, element_dim]\n",
    "\n",
    "        # Prediction mask: all True since decoder always outputs max_elements\n",
    "        pred_mask = torch.ones(B_hat.size(0), B_hat.size(1), dtype=torch.bool, device=B_hat.device)\n",
    "        loss = chamfer_loss(B_hat, B, pred_mask=pred_mask, target_mask=B_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b76a26",
   "metadata": {},
   "source": [
    "# THIS IS A DUMB S2S MAPPER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
